Generate a query to capture data quality issues against the new structured relational data model


Generally speaking I'd use the following framework for QA.

1. What is the fill/null rate for each field?
2. Do all tables have unique values for each primary key and are foreign keys unique where expected?
3. Do the preliminary KPIs that business users are likely to leverage make sense? (In this case I'd query the pricing and spending fields to make sure there are no parsing issues resulting in outliers)
4. Do we have spending/receipt count data for all days or are we missing some due to an outage or other issues?
5. Is the spend/transactions per user on average consistent overtime? Are there any outliers with user level spending that we need to look into?


For the purpose of the assigment, looking into #2, we can see that for some barcodes in the Brands table/json they are mapped to more than 1 entity. The code below will generate the QA check. I uploaded the JSON to Databricks and queried using Pyspark.

from pyspark.sql import functions as F

brands = spark.read.format("json").load("dbfs:/FileStore/shared_uploads/avinash.danda@gmail.com/brands/brands.json")
barcode_check = brands.groupBy("barcode")\
.agg(F.count("*").alias("count"))\
.orderBy(F.col("count").desc())
barcode_check.display()

The output shows that the following barcodes are mapped to 2 entities:

511111204923
511111504788
511111004790
511111605058
511111305125
511111704140
511111504139

This will give incorrect outputs whenever we are trying to join the brand information from the items table to the Brands table as they are only able to be reliably linked with "barcode".
